{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Custom fashion dataset and data pre-processing\n"
      ],
      "metadata": {
        "id": "rjSb7ln5dSB7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwV34BpxrhG4",
        "outputId": "c99b5d05-d3b9-418a-e4f3-2539696de43c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as functional\n",
        "from torch.utils.data import DataLoader\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test=pd.read_csv(\"/content/mnist_test.csv\")\n",
        "train=pd.read_csv(\"/content/mnist_train.csv\")"
      ],
      "metadata": {
        "id": "ab2LNznYuqv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "zj6XUwbgwEH7",
        "outputId": "21e80f99-0377-457a-8b78-792aee611380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
              "0      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "1      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "2      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "3      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "4      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "\n",
              "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
              "0      0      0      0      0      0      0      0      0  \n",
              "1      0      0      0      0      0      0      0      0  \n",
              "2      0      0      0      0      0      0      0      0  \n",
              "3      0      0      0      0      0      0      0      0  \n",
              "4      0      0      0      0      0      0      0      0  \n",
              "\n",
              "[5 rows x 785 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6785ff5f-ec8b-407b-9bac-f776d65e23d5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>1x1</th>\n",
              "      <th>1x2</th>\n",
              "      <th>1x3</th>\n",
              "      <th>1x4</th>\n",
              "      <th>1x5</th>\n",
              "      <th>1x6</th>\n",
              "      <th>1x7</th>\n",
              "      <th>1x8</th>\n",
              "      <th>1x9</th>\n",
              "      <th>...</th>\n",
              "      <th>28x19</th>\n",
              "      <th>28x20</th>\n",
              "      <th>28x21</th>\n",
              "      <th>28x22</th>\n",
              "      <th>28x23</th>\n",
              "      <th>28x24</th>\n",
              "      <th>28x25</th>\n",
              "      <th>28x26</th>\n",
              "      <th>28x27</th>\n",
              "      <th>28x28</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 785 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6785ff5f-ec8b-407b-9bac-f776d65e23d5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6785ff5f-ec8b-407b-9bac-f776d65e23d5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6785ff5f-ec8b-407b-9bac-f776d65e23d5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = train[\"label\"]\n",
        "X_train=train.drop(labels=[\"label\"],axis=1)\n",
        "Y_test=test[\"label\"]\n",
        "X_test=test.drop(labels=\"label\",axis=1)"
      ],
      "metadata": {
        "id": "c-YEnGPruyEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOGtKMq2wKXT",
        "outputId": "be26ba7a-684e-4ca9-fdab-14ffee8026b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=X_train/255.0\n",
        "X_test=X_test/255.0"
      ],
      "metadata": {
        "id": "FLZLbZXlu5nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.tensor(X_train[:10000].values)\n",
        "y_train = torch.tensor(Y_train[:10000].values)\n",
        "\n",
        "# test\n",
        "\n",
        "x_test = torch.tensor(X_test.values)\n",
        "y_test = torch.tensor(Y_test.values)\n",
        "\n",
        "# validate\n",
        "\n",
        "x_validate = torch.tensor(X_train[10000:].values)\n",
        "y_validate = torch.tensor(Y_train[10000:].values)"
      ],
      "metadata": {
        "id": "f-gi1HpnvciV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as functional\n",
        "y_train_one_hot = functional.one_hot(y_train.long(), num_classes = 10)\n",
        "y_test_one_hot = functional.one_hot(y_test.long(), num_classes = 10)\n",
        "y_validate_one_hot = functional.one_hot(y_validate.long(), num_classes = 10)"
      ],
      "metadata": {
        "id": "rUv34O4ZxMCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = TensorDataset(x_train.view(-1, 1, 28, 28), y_train_one_hot)\n",
        "train_data_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "\n",
        "validate_dataset = TensorDataset(x_validate.view(-1, 1, 28, 28), y_validate_one_hot)\n",
        "validate_data_loader = DataLoader(validate_dataset, batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "z8E3d_rcxiCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TensorDataset(x_test.view(-1, 1, 28, 28), y_test_one_hot)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "8LNyyZ89xkSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "data = next(iter(train_data_loader))\n",
        "mean = data[0].mean()\n",
        "std = data[0].std()\n",
        "print(mean, std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWbsiTs_xmTK",
        "outputId": "d8c6d2be-bfac-4f9e-9f1e-eede5f0d37f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1309, dtype=torch.float64) tensor(0.3079, dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(data[0][:100].flatten())\n",
        "plt.axvline(mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "_f4nInPaxoWN",
        "outputId": "790ba364-5050-4866-b291-34a67d55e9f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.lines.Line2D at 0x7fc3497edcc0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (images, labels) in enumerate(validate_data_loader):\n",
        "        # Move tensors to the configured device\n",
        "        images = images.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "        print(images.shape)\n",
        "        print(labels.shape)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNT3i4ILyXnf",
        "outputId": "d6a02624-78a9-4b1e-8c50-491685b7f07d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet image classification using Softmax layer + Cross-Entropy loss on Mnist dataset\n",
        "Note that in PyTorch implementation, the Cross-Entropy loss already has a Softmax layer embedded in itself. Therefore, no Softmax layer was added to the network."
      ],
      "metadata": {
        "id": "j-3LEqj-x3D8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self.make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self.make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self.make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "# Instantiate the ResNet model\n"
      ],
      "metadata": {
        "id": "hPoC4HnUx8b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "learning_rate = 0.005\n",
        "\n",
        "model_softmax =  ResNet18().to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_softmax.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n"
      ],
      "metadata": {
        "id": "3ggerZJ8yBkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "total_step = len(train_data_loader)\n",
        "accuracy_softmax_resnet = []\n",
        "loss_softmax_resnet = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_data_loader):\n",
        "        # Move tensors to the configured device\n",
        "        images = images.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_softmax(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    loss_softmax_resnet.append(loss.item())\n",
        "\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in validate_data_loader:\n",
        "            images = images.float().to(device)\n",
        "            labels = labels.float().to(device)\n",
        "            outputs = model_softmax(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            _, l = torch.max(labels, 1)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            correct += (predicted == l).sum().item()\n",
        "            del images, labels, outputs\n",
        "\n",
        "        accuracy_softmax_resnet.append(100 * correct / total)\n",
        "        print('Accuracy of the network on the {} validation images: {} %'.format(4000, 100 * correct / total))\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print('Execution time:', elapsed_time, 'seconds')\n"
      ],
      "metadata": {
        "id": "aL6Nd0AFyFsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_data_loader:\n",
        "        images = images.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "        outputs = model_softmax(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        _, l = torch.max(labels, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == l).sum().item()\n",
        "        del images, labels, outputs\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D62fPOps_lUf",
        "outputId": "a2426ff4-9d34-4b9c-9d58-cb48b9f56f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 97.91 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet image classification using Softmax layer + Radial loss on Mnist dataset"
      ],
      "metadata": {
        "id": "I_Xgh0zY__E0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet_Softmax(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet_Softmax, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self.make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self.make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self.make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.softmax(out)\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet_softmax18():\n",
        "    return ResNet_Softmax(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "# Instantiate the ResNet model\n"
      ],
      "metadata": {
        "id": "Ln4hKlKvAP6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RadialLoss(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, yHat, y):\n",
        "\n",
        "    # input shape --> (batch, number_of_classes)\n",
        "    #theta_radian = torch.acos(torch.sum(yHat* y, dim=-1) / (torch.norm(yHat, dim=1) * torch.norm(y,dim=1)))\n",
        "    cos_theta = torch.sum(yHat* y, dim=-1) / (torch.norm(yHat, dim=1) * torch.norm(y,dim=1))\n",
        "\n",
        "    return 1 - cos_theta"
      ],
      "metadata": {
        "id": "L1fNGqjkBMvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "learning_rate = 0.005\n",
        "\n",
        "model_radial = ResNet_softmax18().to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = RadialLoss()\n",
        "optimizer = torch.optim.SGD(model_radial.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n"
      ],
      "metadata": {
        "id": "oMaQzNnsBQbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "total_step = len(train_data_loader)\n",
        "accuracy_radial_resnet = []\n",
        "\n",
        "loss_radial_resnet = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_data_loader):\n",
        "        # Move tensors to the configured device\n",
        "        images = images.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_radial(images)\n",
        "        loss = criterion(outputs, labels).mean()\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    loss_radial_resnet.append(loss.item())\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in validate_data_loader:\n",
        "            images = images.float().to(device)\n",
        "            labels = labels.float().to(device)\n",
        "            outputs = model_radial(images)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            _, l = torch.max(labels, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "\n",
        "            correct += (predicted == l).sum().item()\n",
        "            del images, labels, outputs\n",
        "\n",
        "        accuracy_radial_resnet.append(100 * correct / total)\n",
        "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print('Execution time:', elapsed_time, 'seconds')"
      ],
      "metadata": {
        "id": "MnZ1IFyEBU97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_data_loader:\n",
        "        images = images.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "        outputs = model_radial(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        _, l = torch.max(labels, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == l).sum().item()\n",
        "        del images, labels, outputs\n",
        "\n",
        "print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJinaG44tk17",
        "outputId": "354eb891-b39f-4fad-f7c3-d63a54a353b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 5000 validation images: 98.74 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AHMouf4DQSTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet image classification using CLR transformer + Mean Square on Mnist dataset\n"
      ],
      "metadata": {
        "id": "cwkZRW_mQqn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def compute_clr_with_epsilon(tensor, epsilon=0.005):\n",
        "    # Add epsilon only to the zero values\n",
        "    zero_mask = tensor == 0\n",
        "    tensor[zero_mask] += epsilon\n",
        "\n",
        "    # Convert tensor to float data type\n",
        "    tensor = tensor.float()\n",
        "\n",
        "    # Compute the row geometric mean\n",
        "    row_gm = torch.pow(tensor.prod(dim=1), 1 / tensor.shape[1])\n",
        "\n",
        "    # Divide each element by its row geometric mean\n",
        "    tensor_div = tensor / row_gm.unsqueeze(1)\n",
        "\n",
        "    # Take the logarithm (base e) of each element\n",
        "    tensor_log = torch.log(tensor_div)\n",
        "\n",
        "    # Subtract the mean of each column from the corresponding column elements\n",
        "    tensor_clr = tensor_log - tensor_log.mean(dim=0)\n",
        "\n",
        "    return tensor_clr\n"
      ],
      "metadata": {
        "id": "KMMWZAtWQw2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "learning_rate = 0.005\n",
        "\n",
        "model = ResNet18().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n"
      ],
      "metadata": {
        "id": "wMs2eVSVQxdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_one_hot_clr = compute_clr_with_epsilon(y_train_one_hot.float())"
      ],
      "metadata": {
        "id": "1jmO2vxHQxa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataset = TensorDataset(x_train.view(-1, 1, 28, 28), y_train_one_hot_clr)\n",
        "train_data_loader_clr = DataLoader(train_dataset, batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "Al4HbV7SQxYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "total_step = len(train_data_loader)\n",
        "accuracy_clr_resnet = []\n",
        "loss_clr_resnet = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_data_loader_clr):\n",
        "        # Move tensors to the configured device\n",
        "        images = images.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "        #labels = compute_clr_with_epsilon(labels.float().to(device))\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    loss_clr_resnet.append(loss.item())\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in validate_data_loader:\n",
        "            images = images.float().to(device)\n",
        "            labels = labels.float().to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            _, l = torch.max(labels, 1)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            correct += (predicted == l).sum().item()\n",
        "            del images, labels, outputs\n",
        "\n",
        "        accuracy_clr_resnet.append(100 * correct / total)\n",
        "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print('Execution time:', elapsed_time, 'seconds')"
      ],
      "metadata": {
        "id": "yGx2nw32RJRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_data_loader:\n",
        "        images = images.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        _, l = torch.max(labels, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == l).sum().item()\n",
        "        del images, labels, outputs\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haQG9QPTbUzq",
        "outputId": "bc9481c6-87b2-41b8-9a9f-2b1780883386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 99.27 %\n"
          ]
        }
      ]
    }
  ]
}