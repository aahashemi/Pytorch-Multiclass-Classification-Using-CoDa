{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_CoWbTn5ZHfQ"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Custom fashion dataset and data pre-processing"
      ],
      "metadata": {
        "id": "_CoWbTn5ZHfQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79p5KEZ3J3UW",
        "outputId": "9101776e-f1aa-4753-9719-77279eb2bbf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as functional\n",
        "from torch.utils.data import DataLoader\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install the datasated from https://www.kaggle.com/datasets/aahashemi/zalando-fashion-dataset"
      ],
      "metadata": {
        "id": "2tgwM8J8KCqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the csv file\n",
        "df = pd.read_csv(\"zalando_fashion_dataset.csv\")\n",
        "# categories and their corresponding id\n",
        "CATEGORIES = {'Jacket':0, 'Pants':1, 'Jeans':2, 'Shorts':3, 'T-shirt':4,\n",
        "                'Pullover':5, 'Bag':6, 'Cap':7, 'Sandal':8, 'Skirt':9}\n",
        "\n",
        "# split the dataset into train, test, and validation\n",
        "df_train = df.iloc[:40000, 1:786]\n",
        "df_test = df.iloc[40000:45000, 1:786]\n",
        "df_validate = df.iloc[45000:, 1:786]"
      ],
      "metadata": {
        "id": "lSZp9KL7KD1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separate x and y from the dataframes and convert them into Pytorch tensors\n",
        "\n",
        "# train\n",
        "x_train = df_train.iloc[:, 1:786]\n",
        "x_train = torch.tensor(x_train.values)\n",
        "\n",
        "y_train = df_train.iloc[:, 0]\n",
        "y_train = torch.tensor(y_train.values)\n",
        "\n",
        "# test\n",
        "x_test = df_test.iloc[:, 1:786]\n",
        "x_test = torch.tensor(x_test.values)\n",
        "\n",
        "y_test = df_test.iloc[:, 0]\n",
        "y_test = torch.tensor(y_test.values)\n",
        "\n",
        "# validate\n",
        "x_validate = df_validate.iloc[:, 1:786]\n",
        "x_validate = torch.tensor(x_validate.values)\n",
        "\n",
        "y_validate = df_validate.iloc[:, 0]\n",
        "y_validate = torch.tensor(y_validate.values)\n"
      ],
      "metadata": {
        "id": "R3OX1hprKez-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encode the y labels\n",
        "y_train_one_hot = functional.one_hot(y_train.long(), num_classes = 10)\n",
        "y_test_one_hot = functional.one_hot(y_test.long(), num_classes = 10)\n",
        "y_validate_one_hot = functional.one_hot(y_validate.long(), num_classes = 10)\n",
        "\n"
      ],
      "metadata": {
        "id": "G0Thh7rrKg9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_train = torch.mean(x_train, dim=0)\n",
        "var_train = torch.var(x_train, dim=0)\n",
        "\n",
        "mean_test = torch.mean(x_test, dim=0)\n",
        "var_test = torch.var(x_test, dim=0)\n",
        "\n",
        "mean_validate = torch.mean(x_validate, dim=0)\n",
        "var_validate = torch.var(x_validate, dim=0)"
      ],
      "metadata": {
        "id": "NcJAt3MUKjJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_norm = (x_train - mean_train) / torch.sqrt(var_train)\n",
        "x_test_norm = (x_test - mean_test) / torch.sqrt(var_test)\n",
        "x_validate_norm =(x_validate - mean_validate) / torch.sqrt(var_validate)"
      ],
      "metadata": {
        "id": "0fJaJ4IfKrBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = TensorDataset(x_train_norm.view(-1, 1, 28, 28), y_train_one_hot)\n",
        "train_data_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "\n",
        "validate_dataset = TensorDataset(x_validate_norm.view(-1, 1, 28, 28), y_validate_one_hot)\n",
        "validate_data_loader = DataLoader(validate_dataset, batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "wtjBFOlpKu0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TensorDataset(x_test_norm.view(-1, 1, 28, 28), y_test_one_hot)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "oC1Eix_zfenP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "data = next(iter(train_data_loader))\n",
        "mean = data[0].mean()\n",
        "std = data[0].std()\n",
        "print(mean, std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vki9q7a5KyIu",
        "outputId": "053bd8a4-244a-4bd9-9f4a-2929dcf9f86a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.0251, dtype=torch.float64) tensor(0.9567, dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(data[0][:100].flatten())\n",
        "plt.axvline(mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "sRK0noQPK0CJ",
        "outputId": "21cd9df6-2fc2-4016-aa2a-dd264b43ac3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.lines.Line2D at 0x7f9c0f99c0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet image classification using Softmax layer + Cross-Entropy loss on Custom fashion dataset\n",
        "\n",
        "Note that in PyTorch implementation, the Cross-Entropy loss already has a Softmax layer embedded in itself. Therefore, no Softmax layer was added to the network."
      ],
      "metadata": {
        "id": "o-xEXjTWZv7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self.make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self.make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self.make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "# Instantiate the ResNet model\n"
      ],
      "metadata": {
        "id": "6anRJRUhZxTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "learning_rate = 0.005\n",
        "\n",
        "model_softmax =  ResNet18().to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_softmax.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n"
      ],
      "metadata": {
        "id": "iKl89rB0afzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "total_step = len(train_data_loader)\n",
        "accuracy_softmax_resnet = []\n",
        "loss_softmax_resnet = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_data_loader):\n",
        "        # Move tensors to the configured device\n",
        "        images = images.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_softmax(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    loss_softmax_resnet.append(loss.item())\n",
        "\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in validate_data_loader:\n",
        "            images = images.float().to(device)\n",
        "            labels = labels.float().to(device)\n",
        "            outputs = model_softmax(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            _, l = torch.max(labels, 1)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            correct += (predicted == l).sum().item()\n",
        "            del images, labels, outputs\n",
        "\n",
        "        accuracy_softmax_resnet.append(100 * correct / total)\n",
        "        print('Accuracy of the network on the {} validation images: {} %'.format(4000, 100 * correct / total))\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print('Execution time:', elapsed_time, 'seconds')\n"
      ],
      "metadata": {
        "id": "6DP072lMhCjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_data_loader:\n",
        "        images = images.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "        outputs = model_softmax(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        _, l = torch.max(labels, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == l).sum().item()\n",
        "        del images, labels, outputs\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni2zrqwVfS9C",
        "outputId": "cddd3433-3e0d-4cb1-ef3c-0ac8cd735f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 79.4 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet image classification using Softmax layer + Radial loss on Custom fashion dataset"
      ],
      "metadata": {
        "id": "jqiW8Chip1r8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet_Softmax(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet_Softmax, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self.make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self.make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self.make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.softmax(out)\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet_softmax18():\n",
        "    return ResNet_Softmax(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "# Instantiate the ResNet model\n"
      ],
      "metadata": {
        "id": "A5Md5lXIp58v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RadialLoss(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, yHat, y):\n",
        "\n",
        "    # input shape --> (batch, number_of_classes)\n",
        "    #theta_radian = torch.acos(torch.sum(yHat* y, dim=-1) / (torch.norm(yHat, dim=1) * torch.norm(y,dim=1)))\n",
        "    cos_theta = torch.sum(yHat* y, dim=-1) / (torch.norm(yHat, dim=1) * torch.norm(y,dim=1))\n",
        "\n",
        "    return 1 - cos_theta"
      ],
      "metadata": {
        "id": "yWdNeXgorIVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "learning_rate = 0.005\n",
        "\n",
        "model_radial = ResNet_softmax18().to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = RadialLoss()\n",
        "optimizer = torch.optim.SGD(model_radial.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n"
      ],
      "metadata": {
        "id": "4AiPa28OqlFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "total_step = len(train_data_loader)\n",
        "accuracy_radial_resnet = []\n",
        "loss_radial_resnet = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_data_loader):\n",
        "        # Move tensors to the configured device\n",
        "        images = images.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_radial(images)\n",
        "        loss = criterion(outputs, labels).mean()\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    loss_radial_resnet.append(loss.item())\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in validate_data_loader:\n",
        "            images = images.float().to(device)\n",
        "            labels = labels.float().to(device)\n",
        "            outputs = model_radial(images)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            _, l = torch.max(labels, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "\n",
        "            correct += (predicted == l).sum().item()\n",
        "            del images, labels, outputs\n",
        "\n",
        "        accuracy_radial_resnet.append(100 * correct / total)\n",
        "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print('Execution time:', elapsed_time, 'seconds')"
      ],
      "metadata": {
        "id": "B0JX8dopq2WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_data_loader:\n",
        "        images = images.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "        outputs = model_radial(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        _, l = torch.max(labels, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == l).sum().item()\n",
        "        del images, labels, outputs\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbI4OEtDxAzB",
        "outputId": "7de3e1a0-bc42-4371-b4d7-7269669ab8e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 80.32 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet image classification using CLR transformer + Mean Square on Custom fashion dataset"
      ],
      "metadata": {
        "id": "zb9nPcG6tDpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def compute_clr_with_epsilon(tensor, epsilon=0.005):\n",
        "    # Add epsilon only to the zero values\n",
        "    zero_mask = tensor == 0\n",
        "    tensor[zero_mask] += epsilon\n",
        "\n",
        "    # Convert tensor to float data type\n",
        "    tensor = tensor.float()\n",
        "\n",
        "    # Compute the row geometric mean\n",
        "    row_gm = torch.pow(tensor.prod(dim=1), 1 / tensor.shape[1])\n",
        "\n",
        "    # Divide each element by its row geometric mean\n",
        "    tensor_div = tensor / row_gm.unsqueeze(1)\n",
        "\n",
        "    # Take the logarithm (base e) of each element\n",
        "    tensor_log = torch.log(tensor_div)\n",
        "\n",
        "    # Subtract the mean of each column from the corresponding column elements\n",
        "    tensor_clr = tensor_log - tensor_log.mean(dim=0)\n",
        "\n",
        "    return tensor_clr\n"
      ],
      "metadata": {
        "id": "9xiWMzDts8lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "learning_rate = 0.005\n",
        "\n",
        "model = ResNet18().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n"
      ],
      "metadata": {
        "id": "kesInelotIh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_one_hot_clr = compute_clr_with_epsilon(y_train_one_hot.float())"
      ],
      "metadata": {
        "id": "T1rJL82u1X6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataset = TensorDataset(x_train_norm.view(-1, 1, 28, 28), y_train_one_hot_clr)\n",
        "train_data_loader_clr = DataLoader(train_dataset, batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "wsE5KjfY1xa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "total_step = len(train_data_loader)\n",
        "accuracy_clr_resnet = []\n",
        "loss_clr_resnet = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_data_loader_clr):\n",
        "        # Move tensors to the configured device\n",
        "        images = images.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "        #labels = compute_clr_with_epsilon(labels.float().to(device))\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    loss_clr_resnet.append(loss.item())\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in validate_data_loader:\n",
        "            images = images.float().to(device)\n",
        "            labels = labels.float().to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            _, l = torch.max(labels, 1)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            correct += (predicted == l).sum().item()\n",
        "            del images, labels, outputs\n",
        "\n",
        "        accuracy_clr_resnet.append(100 * correct / total)\n",
        "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print('Execution time:', elapsed_time, 'seconds')"
      ],
      "metadata": {
        "id": "sSELQ6uZtW3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_data_loader:\n",
        "        images = images.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        _, l = torch.max(labels, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == l).sum().item()\n",
        "        del images, labels, outputs\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x79mPUR6GFBx",
        "outputId": "a0e14a99-19ba-4892-dd82-457caa9abb2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 80.1 %\n"
          ]
        }
      ]
    }
  ]
}